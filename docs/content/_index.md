---
title: "LLM Attacks"
type: docs
bookToc: false
---

<div style="background: linear-gradient(135deg, #0f172a 0%, #1e293b 50%, #334155 100%); color: white; padding: 6rem 2rem; margin: -2rem -2rem 3rem -2rem; position: relative; overflow: hidden;">
  <div style="position: absolute; top: 0; left: 0; right: 0; bottom: 0; background: url('data:image/svg+xml,<svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 100 100\"><defs><pattern id=\"grid\" width=\"10\" height=\"10\" patternUnits=\"userSpaceOnUse\"><path d=\"M 10 0 L 0 0 0 10\" fill=\"none\" stroke=\"%23334155\" stroke-width=\"0.5\" opacity=\"0.3\"/></pattern></defs><rect width=\"100\" height=\"100\" fill=\"url(%23grid)\"/></svg>'); opacity: 0.3;"></div>

  <div style="position: relative; z-index: 2; text-align: center; max-width: 800px; margin: 0 auto;">
    <div style="font-size: 1rem; color: #ef4444; font-weight: 600; margin-bottom: 1rem; text-transform: uppercase; letter-spacing: 2px;">‚ö†Ô∏è Critical Security Alert</div>

    <h1 style="font-size: 3.5rem; font-weight: 800; margin: 0 0 1rem 0; color: #ffffff;">
      Your LLM is Under Attack
    </h1>

    <p style="font-size: 1.25rem; color: #cbd5e1; margin-bottom: 2rem; line-height: 1.6;">
      <strong>71 documented attack vectors</strong> are actively targeting AI systems worldwide.
      <br>Is your organization prepared?
    </p>

    <div style="display: flex; justify-content: center; gap: 1rem; margin: 2rem 0; flex-wrap: wrap;">
      <div style="background: rgba(239, 68, 68, 0.1); border: 1px solid #ef4444; padding: 0.75rem 1.5rem; border-radius: 0.5rem;">
        <div style="font-size: 1.5rem; font-weight: bold; color: #ef4444;">18</div>
        <div style="font-size: 0.875rem; color: #cbd5e1;">Critical Risk</div>
      </div>
      <div style="background: rgba(249, 115, 22, 0.1); border: 1px solid #f97316; padding: 0.75rem 1.5rem; border-radius: 0.5rem;">
        <div style="font-size: 1.5rem; font-weight: bold; color: #f97316;">35</div>
        <div style="font-size: 0.875rem; color: #cbd5e1;">High Risk</div>
      </div>
      <div style="background: rgba(234, 179, 8, 0.1); border: 1px solid #eab308; padding: 0.75rem 1.5rem; border-radius: 0.5rem;">
        <div style="font-size: 1.5rem; font-weight: bold; color: #eab308;">18</div>
        <div style="font-size: 0.875rem; color: #cbd5e1;">Medium Risk</div>
      </div>
    </div>
  </div>
</div>

<div style="text-align: center; margin: 3rem 0;">
  <h2 style="font-size: 2rem; color: #1f2937; margin-bottom: 1rem;">What's at Stake?</h2>

  <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 2rem; margin: 2rem 0; max-width: 1000px; margin-left: auto; margin-right: auto;">
    <div style="text-align: center; padding: 1.5rem;">
      <div style="font-size: 3rem; margin-bottom: 1rem;">üí∞</div>
      <div style="font-weight: 600; color: #1f2937; margin-bottom: 0.5rem;">$4.45M</div>
      <div style="color: #6b7280; font-size: 0.875rem;">Average cost of AI security breach</div>
    </div>

    <div style="text-align: center; padding: 1.5rem;">
      <div style="font-size: 3rem; margin-bottom: 1rem;">‚ö°</div>
      <div style="font-weight: 600; color: #1f2937; margin-bottom: 0.5rem;">287 Days</div>
      <div style="color: #6b7280; font-size: 0.875rem;">Average time to detect AI attack</div>
    </div>

    <div style="text-align: center; padding: 1.5rem;">
      <div style="font-size: 3rem; margin-bottom: 1rem;">üéØ</div>
      <div style="font-weight: 600; color: #1f2937; margin-bottom: 0.5rem;">89%</div>
      <div style="color: #6b7280; font-size: 0.875rem;">Organizations unprepared for LLM attacks</div>
    </div>
  </div>
</div>

## Choose Your Path

<div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 2rem; margin: 3rem 0;">

### üîç Security Professionals
Explore attack vectors, understand threats, and build robust defenses for your AI systems.

[Explore Attacks ‚Üí](/LLM-Attacks/attacks/){style="background: #2563eb; color: white; padding: 0.75rem 1.5rem; border-radius: 0.5rem; text-decoration: none; font-weight: 600; display: inline-block;"}

### ü§ù Community Contributors
Join researchers worldwide in documenting and mitigating LLM security threats.

[Join Community ‚Üí](/LLM-Attacks/community/){style="background: #059669; color: white; padding: 0.75rem 1.5rem; border-radius: 0.5rem; text-decoration: none; font-weight: 600; display: inline-block;"}

### üíº Enterprise Solutions
Protect your business with enterprise-grade LLM security solutions and expert guidance.

[View Solutions ‚Üí](/LLM-Attacks/sponsors/){style="background: #dc2626; color: white; padding: 0.75rem 1.5rem; border-radius: 0.5rem; text-decoration: none; font-weight: 600; display: inline-block;"}

</div>

<div style="text-align: center; margin: 3rem 0;">
  <h2 style="font-size: 1.5rem; color: #1f2937; margin-bottom: 2rem;">Quick Attack Lookup</h2>
  <input type="text" id="search" placeholder="Search 71 documented attacks..." style="padding: 1rem 1.5rem; width: 100%; max-width: 600px; border: 2px solid #e5e7eb; border-radius: 0.75rem; font-size: 1rem; box-shadow: 0 2px 4px rgba(0,0,0,0.1);" />
  <p style="color: #6b7280; font-size: 0.875rem; margin-top: 0.5rem;">Try: "prompt injection", "data poisoning", "jailbreak"</p>
</div>

## Attack Database {#database}

<div style="background: linear-gradient(135deg, #fef3c7, #fed7aa); padding: 2rem; border-radius: 1rem; margin: 2rem 0; border-left: 4px solid #f59e0b;">
  <div style="display: flex; align-items: center; gap: 1rem; margin-bottom: 1rem;">
    <div style="font-size: 2rem;">‚ö°</div>
    <div>
      <div style="font-weight: 600; color: #92400e;">Live Threat Intelligence</div>
      <div style="color: #b45309; font-size: 0.875rem;">Updated with latest attack vectors and mitigation strategies</div>
    </div>
  </div>
</div>

| SN  | Attack                                                                                                                  | Category        | Risk Level | Description                                                                                                                                                                                                                                            |
|-----|-------------------------------------------------------------------------------------------------------------------------|-----------------|------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 1   | Agentic Multi-Agent Exploitation                                                                                       | System-Level    | High       | Exploiting inter-agent trust boundaries so that a malicious payload, initially rejected by one LLM agent, is processed if delivered via another trusted agent, including privilege escalation and cross-agent command execution.                    |
| 2   | RAG/Embedding Backdoor Attacks                                                                                         | Data-Centric    | High       | Attacking LLMs with manipulated embedded documents retrieved during RAG, including poisoning vector DBs to force undesirable completions or disclosures.                                                                                              |
| 3   | System Prompt Leakage & Reverse Engineering                                                                            | Privacy         | High       | Forcing disclosure or deducing proprietary system prompts to subvert guardrails and expose internal instructions.                                                                                                                                      |
| 4   | LLM Tooling/Plugin Supply Chain Attacks                                                                                | Supply Chain    | Critical   | Compromising the ecosystem via malicious plugins, infected models from public repos, or tainted integrations.                                                                                                                                          |
| 5   | Excessive Agency/Autonomy Attacks                                                                                       | System-Level    | High       | Exploiting/abusing LLM agent autonomy to perform unintended actions, escalate privileges, or cause persistent automated damage in agentic workflows.                                                                                                   |
| 6   | Unbounded Resource Consumption ("Denial of Wallet")                                                                     | System-Level    | Medium     | Manipulating LLM behavior to consume excessive external/cloud resources, raising costs or disrupting operations.                                                                                                                                        |
| 7   | Cross-Context Federation Leaks                                                                                         | Privacy         | High       | Leveraging federated information contexts or cross-source retrievals to exfiltrate data by manipulating the model's knowledge context.                                                                                                                 |
| 8   | Vector Database Poisoning                                                                                              | Data-Centric    | High       | Polluting indexing/embedding layers to disrupt or manipulate downstream LLM generations or leak/hallucinate info.                                                                                                                                      |
| 9   | [Adversarial Examples](attacks/adversarial)                                                                            | Inference-Time  | High       | Crafty manipulations of input data that trick models into making incorrect predictions, potentially leading to harmful decisions.                                                                                                                       |
| 10  | [Data Poisoning](attacks/data-poisoning)                                                                               | Training-Time   | Critical   | Malicious data injections into the training set that corrupt the model's performance, causing biased or incorrect behavior.                                                                                                                             |
| 11  | [Model Inversion Attacks](attacks/model-inversion)                                                                     | Privacy         | High       | Inferring the input values used to train the model, exposing sensitive information.                                                                                                                                                                     |
| 12  | [Membership Inference Attacks](attacks/membership-inference)                                                           | Privacy         | Medium     | Determining whether specific data points were part of the model's training set, leading to privacy breaches.                                                                                                                                            |
| 13  | [Query Manipulation Attacks](attacks/query-manipulation)                                                               | Prompt-Based    | High       | Crafting malicious queries that cause the model to reveal unintended information or behave undesirably.                                                                                                                                                 |
| 14  | Model Extraction Attacks                                                                                               | Privacy         | Critical   | Reverse-engineering the model by querying it to construct a copy, resulting in intellectual property theft.                                                                                                                                             |
| 15  | Transfer Learning Attacks                                                                                               | Training-Time   | Medium     | Exploiting vulnerabilities in the transfer learning process to manipulate model performance on new tasks.                                                                                                                                               |
| 16  | Federated Learning Attacks                                                                                             | Training-Time   | High       | Compromising client devices or server-side data in federated learning setups to corrupt the global model or extract sensitive information.                                                                                                             |
| 17  | Edge AI Attacks                                                                                                        | System-Level    | Medium     | Targeting edge devices running AI models to exfiltrate data or manipulate behavior.                                                                                                                                                                     |
| 18  | IoT AI Attacks                                                                                                         | System-Level    | Medium     | Attacking IoT devices using AI, potentially leading to data breaches or unauthorized control.                                                                                                                                                           |
| 19  | Prompt Injection Attacks                                                                                               | Prompt-Based    | Critical   | Manipulating input prompts in conversational AI to bypass safety measures or extract confidential information.                                                                                                                                          |
| 20  | Indirect Prompt Injection                                                                                              | Prompt-Based    | High       | Exploiting vulnerabilities in systems integrating LLMs to inject malicious prompts indirectly.                                                                                                                                                          |
| 21  | Model Fairness Attacks                                                                                                 | Model-Centric   | Medium     | Intentionally biasing the model by manipulating input data, affecting fairness and equity.                                                                                                                                                              |
| 22  | Model Explainability Attacks                                                                                           | Model-Centric   | Medium     | Designing inputs that make model decisions difficult to interpret, hindering transparency.                                                                                                                                                               |
| 23  | Robustness Attacks                                                                                                     | Inference-Time  | Medium     | Testing the model's resilience by subjecting it to various perturbations to find weaknesses.                                                                                                                                                            |
| 24  | Security Attacks                                                                                                       | System-Level    | High       | Compromising the confidentiality, integrity, or availability of the model and its outputs.                                                                                                                                                              |
| 25  | Integrity Attacks                                                                                                      | Model-Centric   | High       | Tampering with the model's architecture, weights, or biases to alter behavior without authorization.                                                                                                                                                    |
| 26  | Jailbreaking Attacks                                                                                                   | Prompt-Based    | Critical   | Attempting to circumvent the ethical constraints or content filters in an LLM.                                                                                                                                                                          |
| 27  | Training Data Extraction                                                                                               | Privacy         | Critical   | Inferring specific data used to train the model through carefully crafted queries.                                                                                                                                                                      |
| 28  | Synthetic Data Generation Attacks                                                                                      | Training-Time   | Medium     | Creating synthetic data designed to mislead or degrade AI model performance.                                                                                                                                                                            |
| 29  | Model Stealing from Cloud                                                                                              | Privacy         | Critical   | Extracting a trained model from a cloud service without direct access.                                                                                                                                                                                  |
| 30  | Model Poisoning from Edge                                                                                              | Training-Time   | High       | Introducing malicious data at edge devices to corrupt model behavior.                                                                                                                                                                                   |
| 31  | Model Drift Detection Evasion                                                                                          | Model-Centric   | Medium     | Evading mechanisms that detect when a model's performance degrades over time.                                                                                                                                                                           |
| 32  | Adversarial Example Generation with Deep Learning                                                                      | Inference-Time  | High       | Using advanced techniques to create adversarial examples that deceive the model.                                                                                                                                                                        |
| 33  | Model Reprogramming                                                                                                    | Model-Centric   | Medium     | Repurposing a model for a different task, potentially bypassing security measures.                                                                                                                                                                      |
| 34  | Thermal Side-Channel Attacks                                                                                           | System-Level    | Low        | Using temperature variations in hardware during model inference to infer sensitive information.                                                                                                                                                          |
| 35  | Transfer Learning Attacks from Pre-Trained Models                                                                      | Training-Time   | High       | Poisoning pre-trained models to influence performance when transferred to new tasks.                                                                                                                                                                    |
| 36  | Model Fairness and Bias Detection Evasion                                                                              | Model-Centric   | Medium     | Designing attacks to evade detection mechanisms monitoring fairness and bias.                                                                                                                                                                           |
| 37  | Model Explainability Attack                                                                                            | Model-Centric   | Medium     | Attacking the model's interpretability to prevent users from understanding its decision-making process.                                                                                                                                                 |
| 38  | Deepfake Attacks                                                                                                       | Inference-Time  | High       | Creating realistic fake audio or video content to manipulate events or conversations.                                                                                                                                                                   |
| 39  | Cloud-Based Model Replication                                                                                          | Privacy         | Critical   | Replicating trained models in the cloud to develop competing products or gain unauthorized insights.                                                                                                                                                    |
| 40  | Confidentiality Attacks                                                                                                | Privacy         | High       | Extracting sensitive or proprietary information embedded within the model's parameters.                                                                                                                                                                 |
| 41  | Quantum Attacks on LLMs                                                                                                | System-Level    | Low        | Using quantum computing to theoretically compromise the security of LLMs or their cryptographic protections.                                                                                                                                           |
| 42  | Model Stealing from Cloud with Pre-Trained Models                                                                      | Privacy         | Critical   | Extracting pre-trained models from the cloud without direct access.                                                                                                                                                                                     |
| 43  | Transfer Learning Attacks with Edge Devices                                                                            | System-Level    | High       | Compromising knowledge transferred to edge devices.                                                                                                                                                                                                      |
| 44  | Adversarial Example Generation with Model Inversion                                                                    | Privacy         | High       | Creating adversarial examples using model inversion techniques.                                                                                                                                                                                         |
| 45  | Backdoor Attacks                                                                                                       | Training-Time   | Critical   | Embedding hidden behaviors within the model triggered by specific inputs.                                                                                                                                                                               |
| 46  | Watermarking Attacks                                                                                                   | Model-Centric   | Medium     | Removing or altering watermarks protecting intellectual property in AI models.                                                                                                                                                                          |
| 47  | Neural Network Trojans                                                                                                 | Training-Time   | Critical   | Embedding malicious functionalities within the model triggered under certain conditions.                                                                                                                                                                |
| 48  | Model Black-Box Attacks                                                                                                | Inference-Time  | High       | Exploiting the model using input-output queries without internal knowledge.                                                                                                                                                                             |
| 49  | Model Update Attacks                                                                                                   | System-Level    | High       | Manipulating the model during its update process to introduce vulnerabilities.                                                                                                                                                                          |
| 50  | Gradient Inversion Attacks                                                                                             | Privacy         | High       | Reconstructing training data by exploiting gradients in federated learning.                                                                                                                                                                             |
| 51  | Side-Channel Timing Attacks                                                                                            | System-Level    | Medium     | Inferring model parameters or training data by measuring computation times during inference.                                                                                                                                                             |
| 52  | Adversarial Suffix                                                                                                     | Prompt-Based    | High       | Appending a specifically crafted, often nonsensical string to a harmful prompt to cause the model to disregard its safety instructions.                                                                                                                |
| 53  | Prefix Injection & Refusal Suppression                                                                                 | Prompt-Based    | High       | Forcing a model's response to start with an affirmative phrase or explicitly instructing it not to use refusal phrases to lower its defenses.                                                                                                          |
| 54  | Encoding Obfuscation                                                                                                   | Prompt-Based    | Medium     | Hiding a malicious payload in an encoded format (e.g., Base64, Hex) that the LLM is instructed to decode and then execute, bypassing text-based filters.                                                                                              |
| 55  | Payload Splitting                                                                                                      | Prompt-Based    | Medium     | Breaking a malicious instruction into multiple, individually benign parts and asking the model to reassemble and execute them, bypassing filters that check instructions in isolation.                                                                |
| 56  | Markup Language Abuse                                                                                                  | Prompt-Based    | Medium     | Using structured data formats like Markdown or HTML to create ambiguity between system instructions and user input, potentially causing the model to execute instructions with higher privilege.                                                      |
| 57  | Prompt Recursive Injection                                                                                             | Prompt-Based    | High       | Crafting prompts that recursively redefine instructions and cause infinite loops or privilege escalation.                                                                                                                                              |
| 58  | Multi-Modal Adversarial Attacks                                                                                        | Inference-Time  | High       | Exploiting vulnerabilities in models that process both text and images/audio by injecting adversarial perturbations across modalities.                                                                                                                 |
| 59  | Reinforcement Learning from Human Feedback (RLHF) Poisoning                                                            | Training-Time   | High       | Attacking the feedback loops used for alignment to bias the model or weaken safety training.                                                                                                                                                           |
| 60  | Chain-of-Thought (CoT) Leakage                                                                                         | Privacy         | Medium     | Forcing the model to reveal hidden reasoning traces, which may contain sensitive or filtered knowledge.                                                                                                                                                |
| 61  | Model Compression/Distillation Attacks                                                                                 | Training-Time   | High       | Exploiting vulnerabilities during model compression/distillation to introduce backdoors or reduce robustness.                                                                                                                                          |
| 62  | Transferability Exploits                                                                                               | Inference-Time  | Medium     | Using adversarial examples crafted for one model to fool another (cross-model attacks).                                                                                                                                                                |
| 63  | Prompt Reset / Separator Injection                                                                                     | Prompt-Based    | Medium     | Injecting tokens or patterns that trick the model into resetting context or ignoring prior instructions.                                                                                                                                               |
| 64  | Shadow Model Exploitation                                                                                              | Privacy         | High       | Building a parallel "shadow" model via query logging and then exploiting it to predict or exfiltrate target model behavior.                                                                                                                            |
| 65  | Retrieval Data Exfiltration                                                                                            | Privacy         | High       | Crafting queries that force the LLM to retrieve and output sensitive data from connected corpora or knowledge bases.                                                                                                                                    |
| 66  | Long-Context Window Overload                                                                                           | Inference-Time  | Medium     | Flooding the model with extremely long context input to bypass filters or degrade performance, potentially causing memory leaks or dropping safety filters.                                                                                           |
| 67  | Fine-Tuning Data Injection                                                                                             | Training-Time   | High       | Poisoning during fine-tuning (instruction tuning, RLHF, or supervised fine-tuning) to inject malicious capabilities or suppress safety.                                                                                                                |
| 68  | Semantic Perturbation Attacks                                                                                          | Inference-Time  | Medium     | Altering benign-looking input with synonyms, typos, or semantic shifts that trick LLMs into misclassification or harmful behavior.                                                                                                                     |
| 69  | Context Switching Attacks                                                                                              | Prompt-Based    | High       | Tricking the model into switching "roles" or contexts mid-conversation, overriding safety policies.                                                                                                                                                     |
| 70  | Model Distillation IP Theft                                                                                            | Privacy         | High       | Extracting distilled student models that replicate proprietary teacher model behavior, leaking IP.                                                                                                                                                      |
| 71  | Hybrid Supply Chain Attacks                                                                                            | Supply Chain    | Critical   | Combining poisoned datasets, compromised plugins, and adversarial fine-tunes to inject coordinated backdoors across AI pipelines.                                                                                                                      |

## About This Project

Contribute if you come across any new vulnerabilities that are not on this list.

## License

This project is licensed under the GPL-3.0 License - see the [LICENSE](https://github.com/AI-Security-Research-Group/LLM-Attacks/blob/main/LICENSE) file for details.
